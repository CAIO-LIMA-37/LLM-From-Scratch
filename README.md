# LLM-From-Scratch

### Descri√ß√£o
Este reposit√≥rio re√∫ne uma tradu√ß√£o comentada e adaptada do livro **[LLM From Scratch](https://github.com/rasbt/LLMs-from-scratch)**, de **[Sebastian Raschka](https://sebastianraschka.com)**. O material foi produzido em portugu√™s e executado no Google Colab, com o objetivo de desenvolver meus estudos em Modelos Largos de Linguagem (LLMs) para compreender, do zero, como funcionam as LLMs, agora desejo o tornar acess√≠vel para estudantes e profissionais brasileiros.

Cada notebook corresponde a um cap√≠tulo do livro original, contendo:
- Tradu√ß√µes e explica√ß√µes conceituais detalhadas;
- C√≥digos reescritos e comentados em portugu√™s;
- Observa√ß√µes adicionais sobre fundamentos estat√≠sticos e aspectos de IA generativa.

O projeto nasce do interesse de um estat√≠stico em forma√ß√£o que est√° se aprofundando em **IA generativa** e **estat√≠stica te√≥rica-aplicada**, que busca compreender a base matem√°tica e computacional por tr√°s das arquiteturas modernas de linguagem.

> üí° O material tem fins exclusivamente educacionais, e os cr√©ditos autorais pelo conte√∫do original pertencem ao Doutor Sebastian Raschka, autor do livro. T√°mbem agrade√ßo ao laborat√≥rio **[LASSE](https://www.lasse.ufpa.br/pt)**, por proporcionar a oportunidade de desenvolvimento de minhas habilidades nessa √°rea.

---

### Estrutura e Execu√ß√£o

Todos os notebooks foram desenvolvidos no **Google Colab** e podem ser abertos diretamente atrav√©s dos links ‚ÄúOpen in Colab‚Äù presentes abaixo, ou baixados diretamente nas pastas de cada cap√≠tulo deste reposit√≥rio.

- **Cap√≠tulo 1 - Compreendendo Modelos Largos de Linguagem Introdu√ß√£o e Fundamentos** [***ainda ser√° publicado***]:
    - Explica√ß√µes de alto n√≠vel dos conceitos fundamentais por tr√°s dos grandes modelos de linguagem (LLMs);
    - Insights sobre a arquitetura do transformador da qual os LLMs s√£o derivados;
    - Um plano para construir um LLM do zero.

- **Cap√≠tulo 2 - Trabalhando com Dados de Texto**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sxiHGbu077Wj8O4-JzxfdAnh2-3nnenk?usp=sharing):
    - Preparando texto para treinamento em modelos de linguagem de grande porte;
    - Dividindo texto em tokens de palavras e subpalavras;
    - Codifica√ß√£o de pares de bytes como uma forma mais avan√ßada de tokenizar texto;
    - Amostrando exemplos de treinamento com uma abordagem de janela deslizante;
    - Convertendo tokens em vetores que alimentam um modelo de linguagem de grande porte.
  
- **Cap√≠tulo 3 - Codificando Mecanismos de Aten√ß√£o**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1X50vdgIkOnY6Sgap6FoMuCEysFLbAFJk?usp=sharing):
    - Raz√µes para usar mecanismos de aten√ß√£o em redes neurais;
    - Uma estrutura b√°sica de autoaten√ß√£o, evoluindo para um mecanismo de autoaten√ß√£o aprimorado;
    - Um m√≥dulo de aten√ß√£o causal que permite que os LLMs gerem um token por vez;
    - Mascaramento de pesos de aten√ß√£o selecionados aleatoriamente com dropout para reduzir o overfitting;
    - Empilhamento de m√∫ltiplos m√≥dulos de aten√ß√£o causal em um m√≥dulo de aten√ß√£o multicabe√ßa.
    
- **Cap√≠tulo 4 - Implementando um Modelo GPT do zero para gerar texto**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1s0-_-PpUFQZLPGOqrCw7aK6SZQ_E4BcZ?usp=sharing):
    - Codifica√ß√£o de um modelo de linguagem grande (LLM) semelhante ao GPT que pode ser treinado para gerar texto semelhante ao humano;
    - Normaliza√ß√£o de ativa√ß√µes de camadas para estabilizar o treinamento de redes neurais;
    - Adi√ß√£o de conex√µes de atalho em redes neurais profundas;
    - Implementa√ß√£o de blocos transformadores para criar modelos GPT de v√°rios tamanhos;
    - C√°lculo do n√∫mero de par√¢metros e dos requisitos de armazenamento dos modelos GPT.
  
- **Cap√≠tulo 5 - Pr√©-Treinamento em Dados N√£o Rotulados**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-hPj8EfBttqcRZnlg55_TWsB7FtL5tAY?usp=sharing):
    - Calcular as perdas do conjunto de treinamento e valida√ß√£o para avaliar a qualidade do texto gerado pelo LLM durante o treinamento;
    - Implementar uma fun√ß√£o de treinamento e pr√©-treinar o LLM;
    - Salvar e carregar pesos do modelo para continuar treinando um LLM;
    - Carregar pesos pr√©-treinados do OpenAI.
  
- **Cap√≠tulo 6 - Fine-tuning para Classifica√ß√£o**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16DSLc6QWZsfvF7ZsXjLC-DKbeA1izl13?usp=sharing):
    - Introdu√ß√£o de diferentes abordagens de ajuste fino de LLM;
    - Prepara√ß√£o de um conjunto de dados para classifica√ß√£o de texto;
    - Modifica√ß√£o de um LLM pr√©-treinado para ajuste fino;
    - Ajuste fino de um LLM para identificar mensagens de spam;
    - Avalia√ß√£o da precis√£o de um classificador de LLM ajustado;
    - Utiliza√ß√£o de um LLM ajustado para classificar novos dados.
  
- **Cap√≠tulo 7 - Fine-tuning para seguir instru√ß√µes**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nU0Rj4jkmw8fAPv4jOno_t90TKjXAOsL?usp=sharing):
    - O processo de ajuste fino de instru√ß√µes de LLMs;
    - Preparar um conjunto de dados para ajuste fino de instru√ß√µes supervisionado;
    - Organizar dados de instru√ß√µes em lotes de treinamento;
    - Carregar um LLM pr√©-treinado e ajust√°-lo para seguir instru√ß√µes humanas;
    - Extrair respostas de instru√ß√µes geradas por LLM para avalia√ß√£o;
    - Avaliar um LLM com ajuste fino de instru√ß√µes.

